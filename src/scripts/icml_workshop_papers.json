[
    {
        "title": "Minimax Tree of Thoughts: Playing Two-Player Zero-Sum Sequential Games with Large Language Models",
        "abstract": "Large language models are being used to solve an increasing number of tasks, but the existing methods based on large language models are still not good enough in playing two-player zero-sum sequential games. In order to solve the related challenges of large language models playing two-player zero-sum sequential games, we propose Minimax Tree of Thoughts, which combines the idea of Tree of Thoughts and minimax search. Experiment results show that our Minimax Tree of Thoughts method significantly outperforms the original Tree of Thoughts method in two-player zero-sum sequential games tasks such as Word chain and game of Ghost.",
        "session": "Poster Session 1"
    },
    {
        "title": "SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models",
        "abstract": "Spreadsheet manipulation is widely-existing in most daily works and significantly improves the working efficiency. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation, but not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce **SheetRM**, a benchmark featuring long-horizon and multi-category tasks with reasoning dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose **SheetAgent**, an novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: Planner, Informer and Retriever, achieving both advanced reasoning and accurate manipulation over spreadsheets without human interaction through iterative task reasoning and reflection. Extensive experiments demonstrate that SheetAgent delivers 20-30% pass rate improvements on multiple benchmarks over baselines, achieving enhanced precision in spreadsheet manipulation and demonstrating superior table reasoning abilities. More details and visualizations are available at https:\/\/sheetagent.github.io.",
        "session": "Poster Session 1"
    },
    {
        "title": "Matching domain experts by training from scratch on domain knowledge",
        "abstract": "Recently, large language models (LLMs) have outperformed human experts in predicting the results of neuroscience experiments (Luo et al., 2024). What is the basis for this performance? One possibility is that statistical patterns in that specific scientific literature, as opposed to emergent reasoning abilities arising from broader training, underlie LLMs' performance.\nTo evaluate this possibility, we trained (next word prediction) a relatively small 124M-parameter GPT-2 model on 1.3 billion tokens of domain-specific knowledge. Despite being orders of magnitude smaller than larger LLMs trained on trillions of tokens, small models achieved expert-level performance in predicting neuroscience results. Small models trained on the neuroscience literature succeeded when they were trained from scratch using a tokenizer specifically trained on neuroscience text or when the neuroscience literature was used to finetune a pretrained GPT-2. Our results indicate that expert-level performance may be attained by even small LLMs through domain-specific, auto-regressive training approaches.",
        "session": "Poster Session 1"
    },
    {
        "title": "Is persona enough for personality? Using ChatGPT to reconstruct an agent's latent personality from simple descriptions",
        "abstract": "Personality, a fundamental aspect of human cognition, contains a range of traits that influence behaviors, thoughts, and emotions. This paper explores the capabilities of large language models (LLMs) in reconstructing these complex cognitive attributes based only on simple descriptions containing socio-demographic and personality type information. Utilizing the HEXACO personality framework, our study examines the consistency of LLMs in recovering and predicting underlying (latent) personality dimensions from simple descriptions. Our experiments reveal a significant degree of consistency in personality reconstruction, although some inconsistencies and biases, such as a tendency to default to positive traits in the absence of explicit information, are also observed. Additionally, socio-demographic factors like age and number of children were found to influence the reconstructed personality dimensions. These findings have implications for building sophisticated agent-based simulacra using LLMs and highlight the need for further research on robust personality generation in LLMs.",
        "session": "Poster Session 1"
    },
    {
        "title": "Large Language Models Lack Understanding of Character Composition of Words",
        "abstract": "Large language models (LLMs) have demonstrated remarkable performances on a wide range of natural language tasks. Yet, LLMs' successes have been largely restricted to tasks concerning words, sentences, or documents, and it remains questionable how much they understand the minimal units of text, namely characters. In this paper, we examine contemporary LLMs regarding their ability to understand character composition of words, and show that most of them fail to reliably carry out even the simple tasks that can be handled by humans with perfection. We analyze their behaviors with comparison to token level performances, and discuss the potential directions for future research.",
        "session": "Poster Session 1"
    },
    {
        "title": "A Case-based Reasoning Approach to Dynamic Few-Shot Prompting for Code Generation",
        "abstract": "Large language models have recently succeeded in various code generation tasks but still struggle with generating task plans for complex, real-world problems that need detailed, context-aware planning and execution. This work aims to enhance these models' accuracy in generating task plans from natural language instructions. These tasks plans, represented as python code, use custom functions to accomplish the user's request as specified in natural language. The task plans are multi-step, often include loops, and are executed in a python runtime environment. Our approach uses case-based reasoning to perform dynamic few-shot prompting to improve the large language models ability to accurately follow planning prompts. We compare the effectiveness of dynamic prompting with static three-shot and zero-shot prompting approaches finding that dynamic prompting improves the accuracy of the generated code. Additionally, we identify and discuss seven types of failures in code generation.",
        "session": "Poster Session 1"
    },
    {
        "title": "Cognitive Assessment of Language Models",
        "abstract": "Large language models (LLMs) are a subclass of generative artificial intelligence that can interpret language inputs to generate novel responses. These capabilities are conceptualized as a significant step forward in artificial intelligence because the models can seemingly better mimic the thinking and reasoning of human cognition compared to earlier generations of machine learning models that were limited to identifying numeric classes and clusters. Benchmarks for performance are necessary for tracking progress of these models and many existing tasks have served as useful tools. In the current work, we propose a set of such tasks inspired by evidence-based human cognitive assessments from the field of (neuro)psychology and create a battery of questions called the \\textbf{Cognitive Assessments for Language Models (CALM) dataset}. We investigate the capabilities of LLMs to perform in distinct domains of cognitive performance including numeric reasoning, visual spatial reasoning, attention, simple, working, and short term memory, executive functioning, among others. We compare performance across tasks in relation to the size of the LLM. Results demonstrate wide variability in performance in distinct cognitive domains. Of note, the number of parameters was predictive of performance on executive functioning, reasoning, and memory tasks. All models performed strongly at real world reasoning and narrative interpretation tasks. Models universally performed poorly on visual-spatial reasoning tasks.",
        "session": "Poster Session 1"
    },
    {
        "title": "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners",
        "abstract": "This study proposes a hypothesis-testing framework to determine whether large language models (LLMs) possess genuine reasoning abilities or rely on token bias. Carefully-controlled synthetic datasets are generated, and null hypotheses assuming LLMs' reasoning capabilities are tested with statistical guarantees. Inconsistent behavior during experiments leads to the rejection of null hypotheses. Our findings, using the conjunction fallacy as a quintessential example, suggest that current LLMs still struggle with probabilistic reasoning, with apparent performance improvements largely attributable to token bias.",
        "session": "Poster Session 1"
    },
    {
        "title": "Recursive Introspection: Teaching LLM Agents How to Self-Improve",
        "abstract": "A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, to reason and correct their mistakes. In this paper, we introduce $\\textbf{RISE}$: $\\textbf{R}$ecursive $\\textbf{I}$ntro$\\textbf{S}$p$\\textbf{E}$ction, an approach for fine-tuning large language models (LLMs) to enable introspection and self-correction. $\\textbf{RISE}$ prescribes an iterative fine-tuning procedure that teaches the model to alter its response after seeing previously unsuccessful attempts to solve a problem with additional environment feedback. Inspired by online imitation learning, we derive strategies for multi-turn data collection and training to imbue an LLM with the capability to recursively detect and correct its mistakes in subsequent iterations. Experiments show that $\\textbf{RISE}$ enables 7B Llama2 and Mistral models to improve themselves with more turns on math reasoning tasks, outperforming single-turn strategies given equal inference-time computation, without disrupting one-turn abilities.",
        "session": "Poster Session 1"
    },
    {
        "title": "Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations",
        "abstract": "Humans are efficient language learners and inherently social creatures. Our language development is largely shaped by our social interactions, for example, the demonstration and feedback from caregivers. Contrary to human language learning, recent advancements in large language models have primarily adopted a non-interactive training paradigm, and refined pre-trained models through feedback afterward. In this work, we aim to examine how corrective feedback from interactions influences neural language acquisition from the ground up through systematically controlled experiments, assessing whether it contributes to learning efficiency in language models. We introduce a trial-and-demonstration (TnD) learning framework that incorporates three distinct components: student trials, teacher demonstrations, and a reward conditioned on language competence at various developmental stages. Our experiments reveal that the TnD approach accelerates word acquisition for student models of equal and smaller numbers of parameters, and we highlight the significance of both trials and demonstrations. We further show that the teacher's choices of words influence students' word-specific learning efficiency, and a practice-makes-perfect effect is evident by a strong correlation between the frequency of words in trials and their respective learning curves. Our findings suggest that interactive language learning, with teacher demonstrations and active trials, can facilitate efficient word learning in language models.",
        "session": "Poster Session 1"
    },
    {
        "title": "Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Large Language Models",
        "abstract": "The proliferation of large language models has revolutionized natural language processing tasks, yet it raises profound concerns regarding data privacy and security. Language models are trained on extensive corpora including potentially sensitive or proprietary information, and the risk of data leakage --- where the model response reveals pieces of such information --- remains inadequately understood. This study examines susceptibility to data leakage by quantifying the phenomenon of memorization in machine learning models, focusing on the evolution of memorization patterns over training. We reproduce findings that the probability of memorizing a sequence scales logarithmically with the number of times it is present in the data. Furthermore, we find that sequences which are not apparently memorized after the first encounter can be \"uncovered\" throughout the course of training even without subsequent encounters. The presence of these \"latent\" memorized sequences presents a challenge for data privacy since they may be hidden at the final checkpoint of the model. To this end, we develop a diagnostic test for uncovering these latent memorized sequences by considering their cross entropy loss.",
        "session": "Poster Session 1"
    },
    {
        "title": "A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models",
        "abstract": "Recent studies have highlighted their proficiency in some simple tasks like writing and coding through various reasoning strategies. However, LLM agents still struggle with tasks that require comprehensive planning, a process that challenges current models and remains a critical research issue. In this study, we concentrate on travel planning, a Multi-Phases planning problem, that involves multiple interconnected stages, such as outlining, information gathering, and planning, often characterized by the need to manage various constraints and uncertainties. Existing reasoning approaches have struggled to effectively address this complex task.\nOur research aims to address this challenge by developing a human-like planning framework for LLM agents, i.e., guiding the LLM agent to simulate various steps that humans take when solving Multi-Phases problems. Specifically, we implement several strategies to enable LLM agents to generate a coherent outline for each travel query, mirroring human planning patterns. Additionally, we integrate Strategy Block and Knowledge Block into our framework: Strategy Block facilitates information collection, while Knowledge Block provides essential information for detailed planning. Through our extensive experiments, we demonstrate that our framework significantly improves the planning capabilities of LLM agents, enabling them to tackle the travel planning task with improved efficiency and effectiveness. Our experimental results showcase the exceptional performance of the proposed framework; when combined with GPT-4-Turbo, it attains $10\\times$ the performance gains in comparison to the baseline framework deployed on GPT-4-Turbo.",
        "session": "Poster Session 1"
    },
    {
        "title": "Is Self-knowledge and Action Consistent or Not:  Investigating Large Language Model's Personality",
        "abstract": "In this study, we delve into the validity of conventional personality questionnaires in capturing the human-like personality traits of Large Language Models (LLMs). Our objective is to assess the congruence between the personality traits LLMs claim to possess and their demonstrated tendencies in real-world scenarios. By conducting an extensive examination of LLM outputs against observed human response patterns, we aim to understand the disjunction between self-knowledge and action in LLMs.",
        "session": "Poster Session 1"
    },
    {
        "title": "Learning sequence models through consolidation",
        "abstract": "Episodic memory is a reconstructive process, thought to depend on schema-based predictions made by generative models learned through systems consolidation. We extend previous work on memory for static scenes to model the construction and consolidation of sequential experience. After sequences are encoded in the hippocampus, a network is trained to predict the next item in a sequence during replay (simulated by training GPT-2 on a range of stimuli). The resulting model can memorise narratives, with characteristic gist-based distortions, and can also be applied to non-linguistic tasks such as spatial and relational inference. In addition, we explore `retrieval augmented generation', in which sequence generation is conditioned on relevant \u2018memories\u2019, as a model for how hippocampal specifics can be combined with neocortical general knowledge.",
        "session": "Poster Session 1"
    },
    {
        "title": "Code Agents are State of The Art Software Testers",
        "abstract": "Rigorous software testing is crucial for developing and maintaining high-quality code, making automated test generation a promising avenue for both improving software quality and boosting the effectiveness of code generation methods. However, while code generation with Large Language Models (LLMs) is an extraordinarily active research area, test generation remains relatively unexplored. We address this gap and investigate the capability of LLM-based Code Agents for formalizing user issues into test cases. To this end, we propose a novel benchmark based on popular GitHub repositories, containing real-world issues, ground-truth patches, and golden tests. We find that LLMs generally perform surprisingly well at generating relevant test cases with Code Agents designed for code repair, exceeding the performance of systems designed specifically for test generation. Finally, we find that generated tests are an effective filter for proposed code fixes, doubling the precision of SWE-Agent.",
        "session": "Poster Session 1"
    },
    {
        "title": "LLM Sample: part average and part ideal",
        "abstract": "As Large Language Models (LLMs) increasingly impact society, it's crucial to understand the heuristics and biases that drive them.\nWe study the response sampling of LLMs in light of value bias\u2014a tendency to favour high-value options in their outputs. Value bias corresponds to the shift of response from the most likely sample towards some notion of ideal value represented in the LLM.\nOur study identifies value bias in existing and new concepts learned in context. We demonstrate that this bias significantly impacts applications such as patient recovery times. These findings highlight the need to address value bias in LLM deployment to ensure fair and balanced AI applications.",
        "session": "Poster Session 1"
    },
    {
        "title": "From Words to Worlds: Compositionality for Cognitive Architectures",
        "abstract": "Large language models (LLMs) are very performant connectionist systems, but do they exhibit more compositionality? More importantly, is that part of why they perform so well? We present empirical analyses across four LLM families (12 models) and three task categories, including a novel task introduced below. Our findings reveal a nuanced relationship in learning of compositional strategies by LLMs -- while scaling enhances compositional abilities, instruction tuning often has a reverse effect. Such disparity brings forth some open issues regarding the development and improvement of large language models in alignment with human cognitive capacities.",
        "session": "Poster Session 1"
    },
    {
        "title": "Verbalized Machine Learning: Revisiting Machine Learning with Language Models",
        "abstract": "Motivated by the large progress made by large language models (LLMs), we introduce the framework of verbalized machine learning (VML). In contrast to conventional machine learning models that are typically optimized over a continuous parameter space, VML constrains the parameter space to be human-interpretable natural language. Such a constraint leads to a new perspective of function approximation, where an LLM with a text prompt can be viewed as a function parameterized by the text prompt. Guided by this perspective, we revisit classical machine learning problems, such as regression and classification, and find that these problems can be solved by an LLM-parameterized learner and optimizer. The major advantages of VML include (1) easy encoding of inductive bias, (2) automatic model selection and (3) interpretable learner update.",
        "session": "Poster Session 1"
    },
    {
        "title": "Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey",
        "abstract": "Rationality is characterized by logical thinking and decision-making that align with evidence and logical rules. This quality is essential for effective problem-solving, as it ensures that solutions are well-founded and systematically derived. Despite the advancements of large language models (LLMs) in generating human-like text with remarkable accuracy, they present  biases inherited from the training data, inconsistency across different contexts, and difficulty understanding complex scenarios. Therefore, recent research attempts to leverage the strength of multiple agents working collaboratively with various types of data and tools for enhanced consistency and reliability. To that end, this survey aims to define some axioms of rationality, understand whether multi-modal and multi-agent systems are advancing toward rationality, identify their advancements over single-agent, language-only baselines, and discuss open problems and future directions.",
        "session": "Poster Session 1"
    },
    {
        "title": "iWISDM: Assessing instruction following in multimodal models at scale",
        "abstract": "The ability to perform complex tasks from detailed instructions is a key to the remarkable achievements of our species. As humans, we are not only capable of performing a wide variety of tasks but also very complex ones that may entail hundreds or thousands of steps to complete. Large language models and their more recent multimodal counterparts that integrate textual and visual inputs have achieved unprecedented success in performing complex tasks. Yet, most existing benchmarks are largely confined to single-modality inputs \u2014 either text or vision \u2014 and thus, narrowing the scope of multimodal integration assessments, particularly for instruction-following in multimodal contexts. To bridge this gap, we introduce the instructed-Virtual VISual Decision Making (iWISDM) environment engineered to generate a limitless array of vision-language tasks of varying complexity. Using iWISDM, we compiled three distinct benchmarks of instruction following visual tasks across varying complexity levels and evaluated several newly developed multimodal models on these benchmarks. Our findings establish iWISDM as a robust benchmark for assessing the instructional adherence of both existing and emergent multimodal models and highlight a large gap in these models\u2019 ability to precisely follow instructions.",
        "session": "Poster Session 1"
    },
    {
        "title": "Modeling Bilingual Disfluencies with Large Language Models",
        "abstract": "Speech disfluency metrics are commonly used for informing diagnosis and treatment of various communication disorders. However, bilingual speakers exhibit unique speech disfluency patterns, increasing the difficulty of speech and language disorder diagnosis in bilingual children and adults.\nWe propose and train models for predicting disfluencies in monolingual and bilingual speakers, using LLMs and a modern machine learning pipeline. We use a novel bilingual dataset with detailed annotated disfluencies and participant information. We find that disfluencies tend to happen at high surprisal words, validating surprisal theory for both monolinguals and bilinguals. We also find some interesting differences in the manifestation of disfluencies between bilingual and monolingual speakers.",
        "session": "Poster Session 1"
    },
    {
        "title": "Towards Human-AI Collaboration in Healthcare: Guided Deferral Systems with Large Language Models",
        "abstract": "Large language models (LLMs) present a valuable technology for various applications in healthcare, but their tendency to hallucinate introduces unacceptable uncertainty in critical decision-making situations. Human-AI collaboration (HAIC) can mitigate this uncertainty by combining human and AI strengths for better outcomes. This paper presents a novel guided deferral system that provides intelligent guidance when AI defers cases to human decision-makers. We leverage LLMs' verbalisation capabilities and internal states to create this system, demonstrating that fine-tuning smaller LLMs with data from larger models enhances performance while maintaining computational efficiency. A pilot study showcases the effectiveness of our deferral system.",
        "session": "Poster Session 1"
    },
    {
        "title": "Transformers Can Do Arithmetic with the Right Embeddings",
        "abstract": "The poor performance of transformers on arithmetic tasks seems to stem in large part from their inability to keep track of the exact position of each digit inside of a large span of digits. We mend this problem by adding an embedding to each digit that encodes its position relative to the start of the number. In addition to the boost these embeddings provide on their own, we show that this fix enables architectural modifications such as input injection and recurrent layers to improve performance even further.\n\nWith positions resolved, we can study the logical extrapolation ability of transformers. Can they solve arithmetic problems that are larger and more complex than those in their training data? We find that training on only 20 digit numbers with a single GPU for one day, we can reach state-of-the-art performance, achieving up to 99% accuracy on 100 digit addition problems. Finally, we show that these gains in numeracy also unlock improvements on other multi-step reasoning tasks including sorting and multiplication.",
        "session": "Poster Session 1"
    },
    {
        "title": "Position Paper: Dual-System Language Models via Next-Action Prediction",
        "abstract": "In current Large Language Model (LLM) practices, each token is appended sequentially to the output. In contrast, humans are capable of revising and correcting what we write. Inspired by this gap, in this position paper, we propose a dual-system to simultaneously model the thought process and the output process via the introduction of action tokens. This is achieved by (a) maintaining two sequences of tokens, which include a thought system reflecting the human thought process and an output system for storing responses, and (b) introducing removal tokens as action tokens: when a removal token is generated, it is appended only to the thought system, while simultaneously removing certain tokens from the output system. The model uses both systems for next-action prediction. This method allows the retraction of previously generated tokens in the final response and maintains a record of intermediate steps in the thought system. Our framework enables the training of language models to improve the interaction between the thought and output systems, mirroring the way humans refine their thinking for effective written communication. Moreover, it can be implemented with slight modifications to existing LLM architectures and allows for end-to-end training. Finally, we argue that a major limitation in current LLM practice is the absence of data resembling the human thought process, and we advocate for researchers to train language models in a way similar to how humans learn.",
        "session": "Poster Session 1"
    },
    {
        "title": "Training Energy-Efficient Large Language Models Leveraging Equilibrium Driven Bio-Plausible Neural Dynamics",
        "abstract": "Large language Models (LLMs), though growing exceedingly powerful, comprises of orders of magnitude less neurons and synapses than the human brain. However, it requires significantly more power\/energy to operate. In this work, we propose a novel bio-inspired spiking language model (LM) which aims to reduce the computational cost of conventional LMs by drawing motivation from the synaptic information flow in the brain. In this paper, we demonstrate a framework that leverages the average spiking rate of neurons at equilibrium to train a neuromorphic spiking LM using implicit differentiation technique, thereby overcoming the non-differentiability problem of spiking neural network (SNN) based algorithms without using any type of surrogate gradient. The steady-state convergence of the spiking neurons also allows us to design a spiking attention mechanism, which is critical in developing a scalable spiking LM. Moreover, the convergence of average spiking rate of neurons at equilibrium is utilized to develop a novel ANN-SNN knowledge distillation based technique wherein we use a pre-trained BERT model as \"teacher\" to train our \"student\" spiking architecture. Our work is the first one to demonstrate the performance of an operational spiking LM architecture on multiple different tasks in the GLUE benchmark.",
        "session": "Poster Session 1"
    },
    {
        "title": "Improving Self Consistency in LLMs through Probabilistic Tokenization",
        "abstract": "Prior research has demonstrated noticeable performance gains through the use of probabilistic tokenizations, an approach that involves employing multiple tokenizations of the same input string during the training phase of a language model. Despite these promising findings, modern large language models (LLMs) have yet to be trained using probabilistic tokenizations. Interestingly, while the tokenizers of these contemporary LLMs have the capability to generate multiple tokenizations, this property remains underutilized.\n\nIn this work, we propose a novel method to leverage the multiple tokenization capabilities of modern LLM tokenizers, aiming to enhance the self-consistency of LLMs in reasoning tasks. Our experiments indicate that when utilizing probabilistic tokenizations, LLMs generate logically diverse reasoning paths, moving beyond mere surface-level linguistic diversity. We carefully study probabilistic tokenization and offer insights to explain the self consistency improvements it brings through extensive experimentation on 5 LLM families and 4 reasoning benchmarks.",
        "session": "Poster Session 1"
    },
    {
        "title": "AutoGuide: Automated Generation and Selection of Context-Aware Guidelines for Large Language Model Agents",
        "abstract": "Recent advances in large language models (LLMs) have empowered AI agents to perform various sequential decision-making tasks. However, effectively guiding LLMs to perform well in unfamiliar domains like web navigation, where they lack sufficient knowledge, has proven to be difficult with the demonstration-based in-context learning paradigm. In this paper, we introduce a novel framework, called AutoGuide, which addresses this limitation by automatically generating context-aware guidelines from offline experiences. As a result, our guidelines facilitate the provision of relevant knowledge for the agent's current decision-making process. Our evaluation demonstrates that AutoGuide significantly outperforms competitive baselines in complex benchmark domains.",
        "session": "Poster Session 1"
    },
    {
        "title": "Lost in Translation: The Algorithmic Gap Between LMs and the Brain",
        "abstract": "Language Models (LMs) have achieved impressive performance on various linguistic tasks, but their relationship to human language processing in the brain remains unclear. In this paper, we trace gaps and overlaps between LMs and the brain at different levels of analysis, highlighting the importance of probing their internal representations and processes using proper interpretability methods. We discuss how insights from neuroscience, such as sparsity and modularity, internal states, and interactive and continuous learning, can inform the development of more biologically plausible language models. Furthermore, we examine the role of scaling laws in bridging the gap between LMs and human cognition, emphasizing the need for efficiency constraints analogous to those in biological systems.",
        "session": "Poster Session 2"
    },
    {
        "title": "Deep Content Understanding Toward Entity and Aspect Target Sentiment Analysis on Foundation Models",
        "abstract": "Introducing Entity-Aspect Sentiment Triplet Extraction (EASTE), a novel Aspect-Based Sentiment Analysis (ABSA)  which extends Target-Aspect-Sentiment Detection (TASD) by separating aspect categories into pre-defined entities and aspects, keeping the same prediction output but adding additional level of complexity and extending the prediction field. We explore the EASTE solving capabilities of Language models based on transformers architecture from BERT, Flan-T5, Flan-Ul2 to Llama3 and Mixtral, by employing techniques such as zero\/few-shot learning, to Parameter Efficient Fine Tuning (PEFT) such as LoRA: Low-Rank Adaptation (LoRA). The model performances are evaluated on the SamEval-2016 benchmark dataset representing the fair comparison to existing works. Our research not only aims to achieve high performance on the EASTE task but also investigates the impact of model size, type, and adaptation techniques on task performance. Ultimately, we provide detailed insights and achieving state-of-the-art results in complex sentiment analysis.",
        "session": "Poster Session 2"
    },
    {
        "title": "Abstract Understanding of Core-Knowledge Concepts: Humans vs. LLMs",
        "abstract": "The ability to form and use abstractions in a few-shot manner is a key aspect of human cognition; it is this capacity that enables us to understand and act appropriately in novel situations.  In this paper we report on comparisons between humans and GPT-4V on visual tasks designed to systematically assess few-shot abstraction capabilities using core-knowledge concepts related to objectness, object motion, spatial configurations and relationships, and basic numerosity.   We test the impact of presenting tasks to GPT-4V using visual, mixed text-visual, and text-only representations. Our findings highlight that GPT-4V, one of today's most advanced multimodal LLMs, still lacks the flexible intelligence possessed by humans to efficiently relate different situations through novel abstractions.",
        "session": "Poster Session 2"
    },
    {
        "title": "Large Language Models are Bad Game Theoretic Reasoners: Evaluating Performance and Bias in Two-Player Non-Zero-Sum Games",
        "abstract": "Large Language Models (LLMs) are increasingly being used in real-world settings, but their effectiveness in interacting with humans or other AIs remains under-explored. Game theory provides a framework for probing the strategic abilities of LLMs when interacting with other agents. While prior studies have shown that LLMs can solve these tasks with curated prompts, they fail when the problem setting or prompt changes slightly. Therefore, in this work, we study LLMs' behaviour in games such as Stag Hunt and Prisoner Dilemma, analyzing performance variations with different prompts. Our results show that the tested state-of-the-art LLMs are significantly affected by at least one of the following systematic biases; (1) positional bias, (2) payoff bias, or (3) behavioural bias. The significance of this finding is evident in the performance comparison of LLMs when the game configurations are aligned versus misaligned with the affecting bias. Performance is assessed based on the selection of the safer action in both the Stag Hunt and Prisoner's Dilemma (though the metric for evaluating performance is not limited to just the safer action) and alignment refers to whether the LLM's bias aligns with choosing the safer action or not. We found that GPT-3.5, GPT-4, and Llama-3-8B show an average performance drop of 32\\%, 25\\%, and 29\\%, respectively in Stag Hunt, and 28\\%, 16\\%, and 24\\% respectively in Prisoners Dilemma.",
        "session": "Poster Session 2"
    },
    {
        "title": "Understanding the Cognitive Complexity in Language Elicited by Product Images",
        "abstract": "Product images (e.g., a phone) can be used to elicit a diverse set of consumer-reported features expressed through language, including surface-level perceptual attributes (e.g., \"white\") and more complex ones, like perceived utility (e.g., \"battery\"). The cognitive complexity of elicited language reveals the nature of cognitive processes and the context required to understand them; cognitive complexity also predicts consumers' subsequent choices. This work offers an approach for measuring and validating the cognitive complexity of human language elicited by product images, providing a tool for understanding the cognitive processes of human as well as virtual respondents simulated by Large Language Models (LLMs). We introduce a large dataset that includes diverse descriptive labels for product images, including human-rated complexity. We demonstrate that cognitive complexity is correlated with related natural language models and combining models achieves higher alignment with human complexity ratings. Moreover, the minimal supervision required by this approach makes it scalable to use cases with scarce manual complexity ratings. The dataset will be made public.",
        "session": "Poster Session 2"
    },
    {
        "title": "Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning",
        "abstract": "Fine-tuning is the primary methodology for tailoring pre-trained large language models to specific tasks. As the model's scale and the diversity of tasks expand, parameter-efficient fine-tuning methods are of paramount importance. One of the most widely used family of methods is low-rank adaptation (LoRA) and its variants. LoRA encodes weight update as the product of two low-rank matrices. Despite its advantages, LoRA falls short of full-parameter fine-tuning in terms of generalization error for certain tasks. \n\nWe introduce Chain of LoRA (COLA), an iterative optimization framework inspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full parameter fine-tuning,  without incurring additional computational costs or memory overheads. COLA employs a residual learning procedure where it merges learned LoRA modules into the pre-trained language model parameters and re-initialize optimization for new born LoRA modules. We provide theoretical convergence guarantees as well as empirical results to validate the effectiveness of our algorithm. Across various models (OPT and Llama-2) and 11 benchmarking tasks, we demonstrate that COLA can consistently outperform LoRA without additional computational or memory costs.",
        "session": "Poster Session 2"
    },
    {
        "title": "SkillAct: Using Skill Abstractions Improves LLM Agents",
        "abstract": "Complex sequential decision-making tasks often require hierarchical thinking and abstraction: breaking down these tasks into simpler subtasks that can be solved with reusable behaviors, or *skills*.\nIn this work, we show that large language models (LLMs) can benefit from using skill abstractions to solve interactive tasks successfully.\nWe propose a simple prompting approach named **SkillAct**, which can extend existing prompting approaches.\nIn addition, we demonstrate that these skill abstractions can be *learned* from few-shot demonstrations by prompting LLMs.\nWe demonstrate that **SkillAct** improves the performance of existing approaches such as ReAct on the interactive task benchmark ALFWorld.",
        "session": "Poster Session 2"
    },
    {
        "title": "Compositional Communication with LLMs and Reasoning about Chemical Structures",
        "abstract": "Compositionality of communication is considered a prerequisite for reasoning. Despite overall impressive performance, LLMs seem to have fundamental issues with compositionality in reasoning tasks. Research of the emergence of languages in referential games demonstrates that compositionality can be achieved via combination of the game organization and constraints on communication protocols. In this contribution we propose and offer initial evaluation of the hypothesis that compositionality in reasoning tasks with LLMs can be improved by placing LLM agents in the referential games that coax compositionality of the communication. We describe a multi-stage chemical game including recognition, naming, and reconstruction of chemical structures by LLM agents without leveraging their pre-existing chemical knowledge.",
        "session": "Poster Session 2"
    },
    {
        "title": "AI-Assisted Generation of Difficult Math Questions",
        "abstract": "Current LLM training positions mathematical reasoning as a core capability. With publicly available sources fully tapped, there is unmet demand for diverse and challenging mathematics questions. Relying solely on human experts is both time-consuming and costly, while LLM-generated questions often lack the requisite diversity and difficulty.  We present a design framework that combines the strengths of LLMs with a human-in-the-loop approach to generate a diverse array of challenging math questions. Initially, leveraging LLM metacognition skills [Didolkar et al., 2024], a strong LLM is used to extract core \"skills\" from existing math datasets.  These skills serve as the basis for generating novel and difficult questions by prompting the LLM with random pairs of core skills that must be utilized in the question. Our pipeline employs LLMs to iteratively generate and refine questions and solutions through multi-turn prompting. Human annotators then verify and further refine the questions, with their efficiency enhanced through further LLM interactions. Applying this pipeline on skills extracted from MATH dataset [Hendrycks et al., 2021] resulted in a dataset of complex math questions, while improving expert productivity. Despite using skills from the MATH dataset, our approach of combining random skill pairs in questions resulted in noticeably higher quality, as evidenced by lower performance of all models on our questions than on MATH (with open models being the most affected). It can be seen as a method for {\\em scalable oversight,} where human experts evaluate highly capable AI models by also using AI-assistance.",
        "session": "Poster Session 2"
    },
    {
        "title": "Enhancing LLM Complex Reasoning Capability through Hyperbolic Geometry",
        "abstract": "In the era of foundation models and large language models (LLMs), Euclidean space is the de fade facto geometric setting. However, recent studies highlight this choice comes with limitations. We investigate the non-Euclidean characteristics of LLMs on complex reasoning tasks, finding that token embeddings and hidden states exhibit significant hyperbolicity, indicating an underlying hyperbolic structure.\nTo exploit this hyperbolicity, we propose Hyperbolic Low-Rank Adaptation (\\method), which performs low-rank adaptation directly on the hyperbolic manifold, avoiding issues caused by exponential and logarithmic maps when embedding and weight matrices residing in Euclidean space. Experiments show that HoRA significantly improves LLM performance on complex reasoning tasks, with up to 17.30\\% improvement over Euclidean LoRA on the hard-level AQuA dataset.",
        "session": "Poster Session 2"
    },
    {
        "title": "Disjoint Processing Mechanisms of Hierarchical and Linear Grammars in Large Language Models",
        "abstract": "All natural languages contain hierarchical structure. In humans, this structural restriction is neurologically coded: when presented with linear and hierarchical grammars with identical vocabularies, brain areas responsible for language processing are only sensitive to the hierarchical grammar. In this study, we investigate whether such functionally specialized grammar processing regions can emerge in large language models (LLMs) whose processing mechanisms are formed solely from exposure to language corpora. We prompt transformer-based autoregressive LLMs to determine the grammaticality of hierarchical and linear grammars in an in-context-learning setup. First, we discover that models demonstrate higher accuracy, and lower\/comparable surprisals, on hierarchical grammars. Next, we use attribution patching to show that model components processing hierarchical and linear grammars are distinct. Lastly, ablating components for hierarchical\/linear grammars selectively reduces accuracy for the corresponding grammar. Our findings indicate that large-scale text exposure alone can lead to functional specialization in LLMs.",
        "session": "Poster Session 2"
    },
    {
        "title": "Cognitive Flexibility of Large Language Models",
        "abstract": "Cognitive flexibility is a property of cognitive systems which enables success in rapid adaptation to new tasks in quick succession.  We investigate the degree of cognitive flexibility exhibited by several Large Language Models by evaluating them on two neuropsychological tests, the Wisconsin Card Sorting Test and the Letter-Number Test.  Our findings indicate that some Large Language Models fail to switch tasks within the same context window, despite succeeding at these same tasks in distinct context windows, while others are able to flexibly switch tasks.",
        "session": "Poster Session 2"
    },
    {
        "title": "Can Models Learn Skill Composition from Examples?",
        "abstract": "As large language models (LLMs) become increasingly capable, their ability to exhibit *compositional generalization* of skills has garnered significant attention. Yu et al. 2023 recently introduced Skill-Mix evaluation, where models are tasked with composing a short paragraph demonstrating the use of a specified $k$-tuple of language skills. While small models struggled with even $k=3$, larger models like GPT-4 showed reasonable performance with $k=5$ and $6$.\nIn this paper, we employ a setup akin to Skill-Mix to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing a diverse set of language skills\u2014including rhetorical, literary, reasoning, and theory of mind\u2014GPT-4 was used to generate text samples that exhibit random subsets of $k$ skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of $k$, revealed the following findings: 1) Training on combinations of $k=2$ and $3$ skills results in noticeable improvements in the ability to compose texts with $k=4$ and $5$ skills, despite models never having seen such examples during training. 2) When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills.",
        "session": "Poster Session 2"
    },
    {
        "title": "Generation constraint scaling can mitigate hallucination",
        "abstract": "Addressing the issue of hallucinations in large language models (LLMs) is a critical challenge. As the cognitive mechanisms of hallucination have been related to memory, here we explore hallucination for LLM that is enabled with  explicit memory mechanisms. We empirically demonstrate that by simply scaling the readout vector that constrains generation in a memory-augmented LLM decoder,  hallucination mitigation can be achieved in a training-free manner. Our method is geometry-inspired and outperforms a state-of-the-art LLM editing method on the task of generation of Wikipedia-like biography entries both in terms of generation quality and runtime complexity.",
        "session": "Poster Session 2"
    },
    {
        "title": "Thinking Out-of-the-Box: A Comparative Investigation of Human and LLMs in Creative Problem-Solving",
        "abstract": "We explore the creative problem-solving capabilities of modern LLMs in a constrained setting. To this end, we create MacGyver, an automatically generated dataset consisting of 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. Our task is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions.",
        "session": "Poster Session 2"
    },
    {
        "title": "Fine-tuned network relies on generic representation to solve unseen cognitive task",
        "abstract": "Fine-tuning pretrained language models has shown promising results on a wide range of tasks, but when encountering a novel task, do they rely more on generic pretrained representation, or develop brand new task-specific solutions? Here, we fine-tuned GPT-2 on a context-dependent decision-making task, novel to the model but adapted from neuroscience literature. We compared its performance and internal mechanisms to a version of GPT-2 trained from scratch on the same task. Our results show that fine-tuned models depend heavily on pretrained representations, particularly in later layers, while models trained from scratch develop different, more task-specific mechanisms. These findings highlight the advantages and limitations of pretraining for task generalization and underscore the need for further investigation into the mechanisms underpinning task-specific fine-tuning in LLMs.",
        "session": "Poster Session 2"
    },
    {
        "title": "Base-Change at Prediction: Inference-Time Update of Fine-Tuned Models",
        "abstract": "Foundation models play a central role in recent developments of artificial intelligence on both vision and language domains. However, even if a foundation model is powerful enough at the time to be fine-tuned for various tasks, it will be eventually outdated due to its old knowledge or inadequate capability for new tasks, and then a new foundation model will be prepared by re-training the outdated model with updated data. As a result, the various fine-tuned models based on the outdated model also have to keep up with the new foundation model, typically by fine-tuning again the new foundation model for each task, which should be costly if the number of fine-tuned models or the frequency of updates increases. In this paper, with our simplified theoretical framework, we first derive a probabilistic formula for the fine-tuned model of the new foundation model. Then, based on the formula, we propose a method to avoid the fine-tuning of new foundation models, by editing the predictions of the fine-tuned model in direction to the new foundation model. Compared to previous methods, which edit the predictions of the new foundation model instead, our method consistently keeps or improves accuracy of fine-tuned model for various tasks.",
        "session": "Poster Session 2"
    },
    {
        "title": "Humans Linguistically Align to their Conversational Partners, and Language Models Should Too",
        "abstract": "Humankind has honed its language system over thousands of years to engage in statistical learning and form predictions about upcoming input, often based on properties of or prior conversational experience with a specific conversational partner. Large language models, however, do not adapt their language in a user-specific manner. We argue that AI and ML researchers and developers should not ignore this critical component of human language processing, but instead, incorporate it into LLM development, and that doing so will improve LLM conversational performance, as well as users\u2019 perceptions of models on dimensions such as accuracy and task success.",
        "session": "Poster Session 2"
    },
    {
        "title": "Behavioral Bias of Vision-Language Models: A Behavioral Finance View",
        "abstract": "Large Vision-Language Models (LVLMs) are rapidly evolving as researchers incorporate vision modules into Large Language Models (LLMs) to create more human-like models. However, their application in different domains should be carefully evaluated, as their ability may heavily depend on the training process, and harmful biases may occur. In this paper, we study potential behavioral biases of LVLMs from a behavioral finance perspective, an interdisciplinary area that jointly considers finance and psychology. We propose an end-to-end framework, from data collection to new evaluation metrics, to assess LVLM's reasoning capabilities and dynamic behaviors manifested in two established human financial behavioral biases: recency bias and authority bias. Our evaluations find that recent open-source LVLMs such as LLaVA-NeXT Mistral 7B, MobileVLM-V2 7B, Mini-Gemini 7B HD, MiniCPM-Llama3-V 2.5 and Phi-3-vision-128k suffer significantly from these two biases, while the proprietary model GPT-4o is negligibly impacted. This highlights a direction in which open-source models can improve.",
        "session": "Poster Session 2"
    },
    {
        "title": "Are Large Language Models Chameleons?",
        "abstract": "Do large language models (LLMs) have their own worldviews and personality tendencies? Simulations in which an LLM was asked to answer subjective questions were conducted more than 1 million times. Comparison of the responses from different LLMs with real data from the European Social Survey (ESS) suggests that the effect of prompts on bias and variability is fundamental, highlighting major cultural, age, and gender biases. Methods for measuring the difference between LLMs and survey data are discussed, such as calculating weighted means and a new proposed measure inspired by Jaccard similarity. We conclude that it is important to analyze the robustness and variability of prompts before using LLMs to model individual decisions or collective behavior, as their imitation abilities are approximate at best.",
        "session": "Poster Session 2"
    },
    {
        "title": "Towards Bridging Classical and Neural Computation through a Read-Eval-Print Loop",
        "abstract": "Humans rely on step-by-step reasoning to solve new problems, each step guided by the feedback of its effect on a potential solution. For complicated problems, such a sequence of step-by-step interactions might take place between the human and some sort of software system, like a Python interpreter, and the sequence of operations so obtained would then constitute an algorithm to solve a particular class of problems. Based on these ideas, this work proposes a general and scalable method to generate synthetic training data, which we in turn use to teach a Large Language Model to carry out new and previously unseen tasks. By tracing the execution of an algorithm, through careful transformations of the control flow elements, we can produce ``code traces'' containing step-by-step solutions for a range of problems. We empirically verify the usefulness of training on such data, and its superiority to tracing the state changes directly.",
        "session": "Poster Session 2"
    },
    {
        "title": "Baba Is AI: Break the Rules to Beat the Benchmark",
        "abstract": "Humans solve problems by following existing rules and procedures, and also by leaps of creativity to redefine those rules and objectives. To probe these abilities, we developed a new benchmark based on the game Baba Is You where an agent manipulates both objects in the environment and rules, represented by movable tiles with words written on them, to reach a specified goal and win the game. We test three state-of-the-art multi-modal large language models (OpenAI GPT-4o, Google Gemini-1.5-Pro and Gemini-1.5-Flash) and find that they fail dramatically when generalization requires that the rules of the game must be manipulated and combined.",
        "session": "Poster Session 2"
    },
    {
        "title": "Cognitive Modeling with Scaffolded LLMs: A Case Study of Referential Expression Generation",
        "abstract": "To what extent can LLMs be used as part of a cognitive model of language generation? In this paper, we approach this question by exploring a neuro-symbolic implementation of an algorithmic cognitive model of referential expression generation by Dale & Reiter (1995). The symbolic task analysis implementing the generation as an iterative procedure scaffolds symbolic and gpt-3.5-turbo-based modules. We compare this implementation to an ablated model and a one-shot LLM-only baseline on the A3DS dataset (Tsvilodub & Franke, 2023) and find that our hybrid approach is at the same time cognitively plausible and performs well in complex contexts, while allowing for more open-ended modeling of language generation in a larger domain.",
        "session": "Poster Session 2"
    },
    {
        "title": "In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models",
        "abstract": "Recent advancements in artificial intelligence (AI) have led to the development of highly capable large language models (LLMs) demonstrating significant human-like abilities. This work explores the vulnerability of pretrained LLMs to cognitive biases, particularly the A-Not-B error -- a developmental stage for human infants, characterized by the persistence of previously rewarded behavior despite changed conditions that warrant even trivial adaptation. Our investigation reveals that LLMs, akin to human infants, erroneously apply past successful responses to slightly altered contexts. Employing various reasoning tasks, we demonstrate that LLMs are susceptible to the A-Not-B error, with performance deteriorating under inputs crafted to exploit this bias. Notably, smaller models exhibit heightened vulnerability, mirroring the developmental trajectory of human infants. Furthermore, increasing the number of examples before a context change leads to more pronounced failures, highlighting that LLMs are fundamentally pattern-driven and may falter with minor, non-erroneous changes merely in patterns.",
        "session": "Poster Session 2"
    },
    {
        "title": "Language models can emulate certain cognitive profiles: An investigation on the predictive power of modulated surprisal and entropy",
        "abstract": "To date, most investigations on surprisal and entropy effects in reading have been conducted on the group-level, disregarding individual differences. In this work, we revisit the predictive power (PP) of different LMs' surprisal and entropy measures on data of human reading times by incorporating information of language users' cognitive capacities. To do so, we assess the PP of surprisal and entropy estimated from generative LMs on reading data from subjects for which scores from psychometric tests targeting different cognitive domains are available.\n\nSpecifically, we investigate if modulating surprisal and entropy relative to the cognitive scores increases prediction accuracy of reading times, and we examine whether LMs exhibit systematic biases in the prediction of reading times for cognitively high- or low-performing groups, allowing us to investigate what type of psycholinguistic subjects a given LM emulates.\n\nWe find that incorporating cognitive capacities mostly increases PP of surprisal and entropy on reading times, and that individuals performing well in cognitive tests are less sensitive to predictability effects. Finally, our results suggest that the analyzed LMs emulate readers with lower verbal intelligence, but higher capacities in verbal working-memory, and underline the utility of incorporating individual-level information to gain insights into how LMs operate internally.",
        "session": "Poster Session 2"
    },
    {
        "title": "An information-theoretic study of lying in LLMs",
        "abstract": "In cognitive neuroscience, linking behavioral response to brain activity is often useful to explain human cognition. Similarly, the field of interpretability for large language models (LLMs) often uses hidden layer activations to gain understanding about the behavior of LLMs -- for example to find truth representation or to study circuit level phenomena.\nIn this work we investigate LLM cognition leveraging information theory. Specifically, we utilize information theoretic measures to examine the predictive distribution across layers which we access by applying the logit lens to the hidden residual stream activations.\nWe explore the probability of the predicted token as well as the entropy and the KL divergence between intermediate layers and final layer in a model instructed to lie and tell the truth. Our findings show striking differences in these measures in intermediate to late transformer layers when comparing truths to lies on some setups. On other setups the difference is only apparent in the very last layers. We discuss possible explanations for these phenomena.",
        "session": "Poster Session 2"
    },
    {
        "title": "The Pupil Becomes the Master: Eye-Tracking Feedback for Tuning LLMs",
        "abstract": "Large language models often require alignment with explicit human preferences, which can be sparse and costly. We propose a framework to leverage eye-tracking data as an implicit feedback signal to tune LLMs for controlled sentiment generation using Direct Preference Optimization. Our study demonstrates that eye-tracking feedback can be a valuable signal for tuning LLMs. This motivates future research to investigate the impact of eye-tracking feedback on various tasks, highlighting the potential of integrating eye-tracking data with LLMs to improve their performance and alignment with human preferences.",
        "session": "Poster Session 2"
    },
    {
        "title": "Teaching Transformers Causal Reasoning through Axiomatic Training",
        "abstract": "For text-based AI systems to interact in the real world, causal reasoning is an essential skill. Since interventional data is costly to generate, we study to what extent an agent can learn  causal reasoning from passive data. We consider an axiomatic training setup where an agent learns from multiple demonstrations of a causal axiom (or rule), rather than incorporating the axiom as an inductive bias or inferring it from data values. A key question is whether transformers could learn to generalize from the axiom demonstrations to larger and more complex scenarios. Our results, based on a novel axiomatic training scheme, indicate that such generalization is possible. We consider the task of inferring whether a variable causes another variable, given a causal graph structure. We find that a 67 million parameter transformer model, when trained on linear causal chains (along with some variations) can generalize well to new kinds of graphs, including longer causal chains, causal chains with reversed order, and graphs with branching; even when it is not explicitly trained for such settings. Our model performs at par (or better) than many larger language models  such as GPT-4, Gemini Pro, and Phi-3. Overall, the axiomatic training framework provides a new paradigm of learning causal reasoning from passive data that can be used to learn arbitrary axioms.",
        "session": "Poster Session 2"
    },
    {
        "title": "Large Language Models are Not Inverse Thinkers Quite yet",
        "abstract": "Large language models (LLMs) have exhibited significant proficiency in various reasoning tasks, yet their capacity for \"inverse thinking\" remains underexplored. Inverse thinking, inspired by concepts from cognitive science and popularized by figures such as Charlie Munger, involves approaching problems from an opposite perspective, often simplifying complex issues and offering innovative solutions. This paper evaluates the ability of LLMs to comprehend and apply inverse thinking through a series of experiments designed to test theoretical understanding, contextual comprehension, and practical preference in problem-solving scenarios. Our findings indicate that while LLMs demonstrate a basic grasp of inverse thinking, they struggle to consistently apply it in practical contexts, highlighting a nuanced challenge in capturing this cognitive skill within language models. Finally, we discuss the potential directions for future research along this direction and how it can contribute to make better cognitive LLMs.",
        "session": "Poster Session 2"
    },
    {
        "title": "Proving that Cryptic Crossword Clue Answers are Correct",
        "abstract": "Cryptic crossword clues are challenging cognitive tasks, for which new test sets are released on a daily basis by multiple international newspapers.  Each cryptic clue contains both the definition of the answer to be placed in the crossword grid (in common with regular crosswords), and 'wordplay' that _proves_ that the answer is correct (i.e. a human solver can be confident that an answer is correct without needing crossing words to confirm it).  Using an existing cryptic wordplay proving framework (operating on Python proofs created by an LLM), we show that it is possible to distinguish between correct answers and almost-correct ones based upon whether the wordplay 'works'.",
        "session": "Poster Session 2"
    },
    {
        "title": "Anthropocentric reasoning and the possibility of artificial cognition",
        "abstract": "Evaluating the cognitive capacities of large language models (LLMs) requires overcoming not only anthropomorphic but also anthropocentric biases. This article identifies two types of anthropocentric bias that have been neglected: overlooking how various auxiliary factors can impede LLM performance despite competence (Type-I), and dismissing LLM mechanistic strategies that differ from humans' as not genuinely competent (Type-II). Mitigating these biases necessitates an empirically-driven, iterative approach to mapping cognitive tasks to LLM-specific capacities and mechanisms, which can be done by supplementing carefully designed behavioral experiments with mechanistic studies.",
        "session": "Poster Session 2"
    },
    {
        "title": "Self-Cognition in Large Language Models: An Exploratory Study",
        "abstract": "While Large Language Models (LLMs) have achieved remarkable success across various applications, they also raise concerns regarding self-cognition. In this paper, we perform a pioneering study to explore self-cognition in LLMs. Specifically, we first construct a pool of self-cognition instruction prompts to evaluate where an LLM exhibits self-cognition. We further design four principles to assess LLMs' self-cognition abilities. Additionally, we develop a framework that employs LLM-as-a-Judge to assist humans in evaluating and detecting self-cognition. Our study reveals that four of the 48 models on Chatbot Arena\u2014specifically Command R, Claude3-Opus, Llama-3-70b-Instruct, and Reka-core\u2014demonstrate some level of self-cognition. We observe that larger models with more training data exhibit stronger self-cognition and self-awareness. Additionally, we also explore the utility and trustworthiness of LLM in the self-cognition state, revealing that the self-cognition state enhances some specific tasks such as creative writing and exaggeration. We are confident that our work can serve as inspiration for further research to study the self-cognition in LLMs.",
        "session": "Poster Session 2"
    },
    {
        "title": "LLM-Informed Discrete Prompt Optimization",
        "abstract": "The advent of Large Language Models (LLMs) has significantly improved NLP tasks, but their performance depends on effective prompt engineering, where engineers iteratively craft prompts by observing the dynamics of LLMs. With the rising number of LLMs, each trained on different foundations and thus exhibiting different internal sensitivities, prompt engineering has become an increasingly cumbersome task. The solution to these challenges lies in an automated and reliable model capable of suggesting optimized prompts and adapting to various LLMs. Previous works have primarily focused on training learnable vectors or identifying discrete prompts, which were effective for earlier, smaller language models. However, contemporary LLMs require coherent text prompts tailored to their specific training instructions. In this paper, we address this gap by proposing a methodology for training a lightweight model that not only produces legible, optimized prompts but also adapts to different LLMs. The proposed methodology has demonstrated significant performance improvements with optimized prompts across different LLMs.",
        "session": "Poster Session 2"
    },
    {
        "title": "CogErgLLM: Exploring Large Language Model Systems Design Perspective Using Cognitive Ergonomics",
        "abstract": "Integrating cognitive ergonomics with LLMs is essential for enhancing safety, reliability, and user satisfaction in human-AI interactions. Current LLM design often lacks this integration, leading to systems that may not fully align with human cognitive capabilities and limitations. Insufficient focus on incorporating cognitive science methods exacerbates biases in LLM outputs, while inconsistent application of user-centered design principles results in suboptimal user experiences. To address these challenges, our position paper explores the critical integration of cognitive ergonomics principles into LLM design, aiming to provide a comprehensive framework and practical guidelines for ethical LLM development. The framework encompasses nine vital components, including user-centric design, ergonomic data integration, cognitive load management, user interface design, trust and transparency, feedback mechanisms, ethical considerations, personalization and adaptations, and continuous evaluation and improvement. Additionally, we discuss prototyping procedures, case studies, future opportunities and identify related technical and ethical challenges. Through our contributions, we seek to advance understanding and practice in integrating cognitive ergonomics into LLM systems, fostering safer, more reliable, and ethically sound human-AI interactions.",
        "session": "Poster Session 2"
    },
    {
        "title": "What can VLMs Do for Zero-shot Embodied Task Planning?",
        "abstract": "Recent advances in Vision Language Models (VLMs) for robotics demonstrate their enormous potential. However, for embodied task planning requiring high precision and reliability, the performance limitations of the VLMs remain ambiguous, which greatly constrains their potential application in embodied task planning. To this end, this paper provides an in-depth and comprehensive evaluation of the VLMs' performance in zero-shot embodied task planning. Firstly, we develop a systematic evaluation framework encompassing various dimensions of capabilities essential for task planning for the first time, aiming to answer what factors constrain the VLMs to output accurate task planning. Then, based on the evaluation framework, we propose a benchmark dataset called ETP-Bench to evaluate the performance of VLMs on embodied task planning. Through extensive experiments, the results indicate that the current state-of-the-art VLM, GPT-4V, has only 19\\% accuracy in task planning on our benchmark, and the main factors contributing to its low accuracy are deficiencies in spatial perception and object type recognition. We hope this study can provide data support and inspire more specific research directions for future robotics research.",
        "session": "Poster Session 2"
    },
    {
        "title": "Theory of Mind Assay of Multimodal LLMs",
        "abstract": "The concept of artificial general intelligence (AGI) has sparked intense debates across various sectors, fueled by the capabilities of Large Language Model-based AI systems like ChatGPT. However, the AI community remains divided on whether such models truly understand language and its contexts. Developing multimodal AI systems, which can engage with the user in multiple input and output modalities, is seen as a crucial step towards AGI. We employ iterative Theory of Mind tests to reveal limitations of current multimodal LLMs like ChatGPT 4o in converging to coherent and unified internal world models which results in illogical and inconsistent user interactions both within and across the different input and output modalities. We also identify new multimodal confabulations (\"hallucinations\"), particularly in languages with less training data, such as Bengali.",
        "session": "Poster Session 2"
    }
]